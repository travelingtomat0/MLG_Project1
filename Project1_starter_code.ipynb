{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries that are required to run your project\n",
    "# You are allowed to add more libraries as you need\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.1 - Modeling Choices & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Keys: Index(['gene_name', 'chr', 'gene_start', 'gene_end', 'TSS_start', 'TSS_end',\n",
      "       'strand'],\n",
      "      dtype='object')\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading DNase Matrix: 100%|██████████████| 14310/14310 [01:15<00:00, 189.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14310, 400)\n",
      "Dumping DNase with pickle.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading H3K27ac Matrix: 100%|████████████| 14310/14310 [01:22<00:00, 172.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14310, 400)\n",
      "Dumping H3K27ac with pickle.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading H3K4me1 Matrix: 100%|████████████| 14310/14310 [01:40<00:00, 142.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14310, 400)\n",
      "Dumping H3K4me1 with pickle.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading H3K4me3 Matrix: 100%|████████████| 14310/14310 [01:17<00:00, 183.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14310, 400)\n",
      "Dumping H3K4me3 with pickle.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading H3K36me3 Matrix: 100%|███████████| 14310/14310 [01:31<00:00, 157.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14310, 400)\n",
      "Dumping H3K36me3 with pickle.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading DNase Matrix: 100%|████████████████| 1974/1974 [00:11<00:00, 179.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 400)\n",
      "Dumping DNase with pickle.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading H3K27ac Matrix: 100%|██████████████| 1974/1974 [00:11<00:00, 168.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 400)\n",
      "Dumping H3K27ac with pickle.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading H3K4me1 Matrix: 100%|██████████████| 1974/1974 [00:14<00:00, 133.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 400)\n",
      "Dumping H3K4me1 with pickle.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading H3K4me3 Matrix: 100%|██████████████| 1974/1974 [00:11<00:00, 177.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 400)\n",
      "Dumping H3K4me3 with pickle.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading H3K36me3 Matrix: 100%|█████████████| 1974/1974 [00:12<00:00, 152.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 400)\n",
      "Dumping H3K36me3 with pickle.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading DNase Matrix: 100%|█████████████████| 1974/1974 [00:21<00:00, 92.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 400)\n",
      "Dumping DNase with pickle.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading H3K27ac Matrix: 100%|██████████████| 1974/1974 [00:12<00:00, 156.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 400)\n",
      "Dumping H3K27ac with pickle.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading H3K4me1 Matrix: 100%|██████████████| 1974/1974 [00:17<00:00, 112.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 400)\n",
      "Dumping H3K4me1 with pickle.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading H3K4me3 Matrix: 100%|██████████████| 1974/1974 [00:11<00:00, 167.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 400)\n",
      "Dumping H3K4me3 with pickle.\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading H3K36me3 Matrix: 100%|█████████████| 1974/1974 [00:13<00:00, 149.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1974, 400)\n",
      "Dumping H3K36me3 with pickle.\n",
      "DNase has been loaded from disk.\n",
      "True\n",
      "H3K27ac has been loaded from disk.\n",
      "True\n",
      "H3K4me1 has been loaded from disk.\n",
      "True\n",
      "H3K4me3 has been loaded from disk.\n",
      "True\n",
      "H3K36me3 has been loaded from disk.\n",
      "True\n",
      "DNase has been loaded from disk.\n",
      "True\n",
      "H3K27ac has been loaded from disk.\n",
      "True\n",
      "H3K4me1 has been loaded from disk.\n",
      "True\n",
      "H3K4me3 has been loaded from disk.\n",
      "True\n",
      "H3K36me3 has been loaded from disk.\n",
      "True\n",
      "Number of NaNs in matrix: 2361\n",
      "Number of NaNs in matrix: 98\n",
      "Number of NaNs in matrix: 2678\n",
      "Number of NaNs in matrix: 136\n",
      "Number of NaNs in matrix: 0\n"
     ]
    }
   ],
   "source": [
    "# TODO: \n",
    "# Load your feature (bed and/or bigwig and/or fasta) and target files (tsv) here.\n",
    "# Decide which features to use for training. Feel free to process them however you need.\n",
    "\n",
    "from utils.dataset import *\n",
    "from torch.nn import AvgPool1d\n",
    "\n",
    "# NOTE: \n",
    "# bed and bigwig files contain signals of all chromosomes (including sex chromosomes).\n",
    "# Training and validation split based on chromosomes has been done for you. \n",
    "# However, you can resplit the data in any way you want.\n",
    "\n",
    "#phils_path = \"/home/phil/Downloads/ML4G_Project_1_Data\"\n",
    "\n",
    "path_data = \"/home/phil/Downloads/ML4G_Project_1_Data\"  # TODO\n",
    "path_test = \"/path/to/test/info/file\"   # X3_test_info.tsv ; TODO\n",
    "path_test = os.path.join(path_data, 'CAGE-train/CAGE-train/X3_test_info.tsv')\n",
    "test_genes = pd.read_csv(path_test, sep='\\t')\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "# load gene information\n",
    "train_info_X1 = pd.read_csv(os.path.join(path_data, 'CAGE-train', 'CAGE-train', 'X1_train_info.tsv'), sep='\\t')\n",
    "train_info_X2 = pd.read_csv(os.path.join(path_data, 'CAGE-train', 'CAGE-train', 'X1_train_info.tsv'), sep='\\t')\n",
    "print(f'Training Data Keys: {train_info_X1.keys()}')\n",
    "\n",
    "# Modalities chosen by looking at the\n",
    "modalities = ['DNase', 'H3K27ac', 'H3K4me1', 'H3K4me3', 'H3K36me3']\n",
    "window_size = 20000\n",
    "\n",
    "# prepare data (binning)\n",
    "\"\"\"train_data_X1 = InputDataset(data_directory=path_data, cell_line='X1', objective=\"train\", modality_names=modalities, window_size=window_size, only_bw=True, dim_reduction=AvgPool1d)\n",
    "\n",
    "val_data_X1 = InputDataset(data_directory=path_data, cell_line='X1', objective=\"val\", modality_names=modalities, window_size=window_size, only_bw=True, dim_reduction=AvgPool1d)\n",
    "\n",
    "train_data_X2 = InputDataset(data_directory=path_data, cell_line='X2', objective=\"train\", modality_names=modalities, window_size=window_size, only_bw=False)\n",
    "\n",
    "val_data_X2 = InputDataset(data_directory=path_data, cell_line='X2', objective=\"val\", modality_names=modalities, window_size=window_size, only_bw=False)\"\"\"\n",
    "Bigwig_Matrix_Builder(data_directory='/home/phil/Downloads/ML4G_Project_1_Data', cell_line='X1', modality_names=modalities, window_size=20000, dim_reduction=True, num_bins=400, type='combined')\n",
    "Bigwig_Matrix_Builder(data_directory='/home/phil/Downloads/ML4G_Project_1_Data', cell_line='X1', modality_names=modalities, window_size=20000, dim_reduction=True, num_bins=400, type='combined', val=True)\n",
    "Bigwig_Matrix_Builder(data_directory='/home/phil/Downloads/ML4G_Project_1_Data', cell_line='X2', modality_names=modalities, window_size=20000, dim_reduction=True, num_bins=400, type='combined', val=True)\n",
    "Bigwig_Matrix_Builder(data_directory='/home/phil/Downloads/ML4G_Project_1_Data', cell_line='X2', modality_names=modalities, window_size=20000, dim_reduction=True, num_bins=400, type='combined')\n",
    "Bigwig_Matrix_Builder(data_directory='/home/phil/Downloads/ML4G_Project_1_Data', cell_line='X3', modality_names=modalities, window_size=20000, dim_reduction=True, num_bins=400, type='combined', objective='test')\n",
    "\n",
    "# load_data\n",
    "r = RegressionData(data_path='/home/phil/Downloads/ML4G_Project_1_Data', modalities=['DNase', 'H3K27ac', 'H3K4me1', 'H3K4me3', 'H3K36me3'], window_size=20000, num_bins=400, cell_line='X1', type='combined')\n",
    "r2 = RegressionData(data_path='/home/phil/Downloads/ML4G_Project_1_Data', modalities=['DNase', 'H3K27ac', 'H3K4me1', 'H3K4me3', 'H3K36me3'], window_size=20000, num_bins=400, cell_line='X2', type='combined')\n",
    "r3 = RegressionData(data_path='/home/phil/Downloads/ML4G_Project_1_Data', modalities=['DNase', 'H3K27ac', 'H3K4me1', 'H3K4me3', 'H3K36me3'], window_size=20000, num_bins=400, cell_line='X3', type='combined', objective='test')\n",
    "# ---------------------------------------------------------------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.2 - Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7616842767193518\n"
     ]
    }
   ],
   "source": [
    "# TODO: \n",
    "# Select the best model to predict gene expression from the obtained features in WP 1.1.\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from utils.lightning_wrapper import ModelWrapper\n",
    "from models.neural_nets import *\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "from utils.dataset import RegressionData\n",
    "from utils.dataset import SimpleDataset\n",
    "from models.neural_nets import BinnedConvolutionalModel\n",
    "from utils.lightning_wrapper import ModelWrapper\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "# d1 = SimpleDataset(r.matrix, r.y)\n",
    "# d2 = SimpleDataset(r.val_matrix, r.y_val)\n",
    "\n",
    "d1 = SimpleDataset(np.vstack((r.matrix, r2.matrix)), np.concatenate((r.y, r2.y)))\n",
    "d2 = SimpleDataset(np.vstack((r.val_matrix, r2.val_matrix)), np.concatenate((r.y_val, r2.y_val)))\n",
    "#d = SimpleDataset(r.val_matrix, r.y_val)\n",
    "\n",
    "stepsize = 1\n",
    "data_indices = list(range(0, len(d1), stepsize))\n",
    "sd1 = torch.utils.data.Subset(d1, data_indices)\n",
    "\n",
    "data_indices = list(range(0, len(d2), stepsize))\n",
    "sd2 = torch.utils.data.Subset(d2, data_indices)\n",
    "\n",
    "\"\"\"model = BinnedConvolutionalModel()\n",
    "lightning_model = ModelWrapper(model_architecture=model, learning_rate=1e-3, loss=torch.nn.L1Loss(), \n",
    "                               datasets=[sd1, sd2, sd2], batch_size=1)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=10, deterministic=True,)# reload_dataloaders_every_n_epochs=5)\n",
    "if torch.cuda.is_available():\n",
    "    trainer.fit(lightning_model, accelerator=\"gpu\")\n",
    "else:\n",
    "    trainer.fit(lightning_model)\n",
    "\n",
    "p_dl = np.array(torch.cat(trainer.predict()))\n",
    "\"\"\"\n",
    "\n",
    "# --- Regression ---\n",
    "clf = Lasso(alpha=541)\n",
    "#clf.fit(np.vstack((r.matrix, r2.matrix)), np.concatenate((r.y[:-10], r2.y)))\n",
    "clf.fit(d1.X, d1.y)\n",
    "# clf.fit(new_X, r.y[:-10]+1)\n",
    "p = clf.predict(d2.X)\n",
    "c = spearmanr(p, d2.y).correlation\n",
    "\n",
    "clf.fit(np.vstack((d1.X, d2.X)), np.concatenate((d1.y, d2.y)))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Work Package 1.3 - Prediction on Test Data (Evaluation Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Using the model trained in WP 1.2, make predictions on the test data (chr 1 of cell line X3).\n",
    "# Store predictions in a variable called \"pred\" which is a numpy array.\n",
    "\n",
    "pred = None\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "# pred = np.array(torch.cat(trainer.predict()))\n",
    "pred = clf.predict(r3.matrix)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Check if \"pred\" meets the specified constrains\n",
    "assert isinstance(pred, np.ndarray), 'Prediction array must be a numpy array'\n",
    "assert np.issubdtype(pred.dtype, np.number), 'Prediction array must be numeric'\n",
    "assert pred.shape[0] == len(test_genes), 'Each gene should have a unique predicted expression'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Predictions in the Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions in a ZIP. \n",
    "# Upload this zip on the project website under \"Your submission\".\n",
    "# Zip this notebook along with the conda environment (and README, optional) and upload this under \"Your code\".\n",
    "\n",
    "save_dir = '/home/phil/Downloads'  # TODO\n",
    "file_name = 'gex_predicted.csv'         # PLEASE DO NOT CHANGE THIS\n",
    "zip_name = \"Toma_Philip_Project1.zip\" # TODO\n",
    "save_path = f'{save_dir}/{zip_name}'\n",
    "compression_options = dict(method=\"zip\", archive_name=file_name)\n",
    "\n",
    "test_genes['gex_predicted'] = pred.tolist()\n",
    "test_genes[['gene_name', 'gex_predicted']].to_csv(save_path, compression=compression_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
